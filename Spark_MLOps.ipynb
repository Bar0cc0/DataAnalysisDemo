{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing search terms on an e-commerce web server\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: findspark in /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Install spark\n",
    "!pip install pyspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/28 15:10:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/28 15:10:47 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Start session\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating a spark context class\n",
    "sc = SparkContext()\n",
    "\n",
    "# Creating a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Capstone SparkML lab\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-06-28 15:31:40--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 233457 (228K) [text/csv]\n",
      "Saving to: ‘searchterms.csv.2’\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 21% 26.1M 0s\n",
      "    50K .......... .......... .......... .......... .......... 43% 32.7M 0s\n",
      "   100K .......... .......... .......... .......... .......... 65% 38.3M 0s\n",
      "   150K .......... .......... .......... .......... .......... 87% 40.9M 0s\n",
      "   200K .......... .......... .......                         100% 28.8M=0.007s\n",
      "\n",
      "2023-06-28 15:31:40 (32.8 MB/s) - ‘searchterms.csv.2’ saved [233457/233457]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Get data\n",
    "wget 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/searchterms.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the csv into a spark dataframe\n",
    "import os\n",
    "data_csv = os.environ.get('searchterms', 'searchterms.csv')\n",
    "df = spark.read.csv(data_csv, header=True)\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows = 10000, columns = 4\n"
     ]
    }
   ],
   "source": [
    "# Print the number of rows and columns\n",
    "print(f'rows = {df.count()}, columns = {len(df.columns)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----+--------------+\n",
      "|day|month|year|    searchterm|\n",
      "+---+-----+----+--------------+\n",
      "| 12|   11|2021| mobile 6 inch|\n",
      "| 12|   11|2021| mobile latest|\n",
      "| 12|   11|2021|   tablet wifi|\n",
      "| 12|   11|2021|laptop 14 inch|\n",
      "| 12|   11|2021|     mobile 5g|\n",
      "+---+-----+----+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the top 5 rows\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out the datatype of the column searchterm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('searchterm', 'string')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many times was the term `gaming laptop` searched?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(df.searchterm == 'gaming laptop').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the top 5 most frequently used search terms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:============================================>         (166 + 8) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(searchterm='mobile 5g', count=2301)\n",
      "Row(searchterm='ebooks data science', count=410)\n",
      "Row(searchterm='mobile 6 inch', count=2312)\n",
      "Row(searchterm='tablet 10 inch', count=715)\n",
      "Row(searchterm='laptop', count=935)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def top_terms(df=df, col='searchterm', top=5):\n",
    "    l = df.groupBy('searchterm').count().collect()\n",
    "    [print(l[x]) for x in range(top)]\n",
    "top_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2023-06-28 16:13:31--  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.tar.gz\n",
      "Resolving cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)... 169.63.118.104\n",
      "Connecting to cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud (cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud)|169.63.118.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1490 (1.5K) [application/x-tar]\n",
      "Saving to: ‘model.tar.gz’\n",
      "\n",
      "     0K .                                                     100% 11.3M=0s\n",
      "\n",
      "2023-06-28 16:13:32 (11.3 MB/s) - ‘model.tar.gz’ saved [1490/1490]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Get pretrained sales forecasting model\n",
    "wget  https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-DB0321EN-SkillsNetwork/Bigdata%20and%20Spark/model.tar.gz\n",
    "tar -xzf model.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the sales forecast model.\n",
    "from pyspark.ml.regression import LinearRegressionModel\n",
    "model = LinearRegressionModel.load('sales_prediction.model')\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the sales forecast model, predict the sales for the year of 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\\nelasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\\nepsilon: The shape parameter to control the amount of robustness. Must be > 1.0. (default: 1.35)\\nfeaturesCol: features column name (current: features)\\nfitIntercept: whether to fit an intercept term (default: True)\\nlabelCol: label column name (current: sales)\\nloss: The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError) (default: squaredError)\\nmaxIter: maximum number of iterations (>= 0) (current: 100)\\npredictionCol: prediction column name (default: prediction)\\nregParam: regularization parameter (>= 0) (current: 0.1)\\nsolver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (Default auto) (default: auto)\\nstandardization: whether to standardize the training features before fitting the model (default: True)\\ntol: the convergence tolerance for iterative algorithms (>= 0) (default: 1e-06)\\nweightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0 (undefined)'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----+\n",
      "|          intercept|       coefficients|scale|\n",
      "+-------------------+-------------------+-----+\n",
      "|-13019.989140447298|[6.522567861288859]|  1.0|\n",
      "+-------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet('./sales_prediction.model/data/part-00000-1db9fe2f-4d93-4b1f-966b-3b09e72d664e-c000.snappy.parquet')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(year):\n",
    "    assembler = VectorAssembler(inputCols=[\"year\"],outputCol=\"features\")\n",
    "    data = [[year,0]]\n",
    "    columns = [\"year\", \"sales\"]\n",
    "    _ = spark.createDataFrame(data, columns)\n",
    "    __ = assembler.transform(_).select('features','sales')\n",
    "    predictions = model.transform(__)\n",
    "    predictions.select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        prediction|\n",
      "+------------------+\n",
      "|175.16564294006457|\n",
      "+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/28 16:39:51 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/06/28 16:39:51 WARN netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "predict(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
